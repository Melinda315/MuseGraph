{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Dataset",
   "id": "dd6fd313df9be805"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "Index = []\n",
    "Name = {}\n",
    "Type = {}\n",
    "Label = {}\n",
    "\n",
    "train_Index = []\n",
    "test_Index = []\n",
    "\n",
    "count=0\n",
    "flag=False\n",
    "with open('./node.dat','r') as file:\n",
    "    for line in file:\n",
    "        count+=1\n",
    "        temp = line[:-1].split('\\t')\n",
    "\n",
    "        if len(temp)!=4 and flag==False:\n",
    "            print(temp)\n",
    "            print(count)\n",
    "            flag=True\n",
    "            \n",
    "        Index.append(int(temp[0]))\n",
    "        Name[int(temp[0])] = temp[1]\n",
    "        Type[int(temp[0])] = int(temp[2])"
   ],
   "id": "e909f913799e9a45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('./label.dat','r') as file:\n",
    "    for line in file:\n",
    "        temp = line[:-1].split('\\t')\n",
    "        train_Index.append(int(temp[0]))\n",
    "        Label[int(temp[0])] = temp[3].split(',')\n",
    "        Label[int(temp[0])].sort()\n"
   ],
   "id": "83a95b7a4d75c1fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open('./label.dat.test','r') as file:\n",
    "    for line in file:\n",
    "        temp = line[:-1].split('\\t')\n",
    "        test_Index.append(int(temp[0]))\n",
    "        Label[int(temp[0])] = temp[3].split(',')\n",
    "        Label[int(temp[0])].sort()\n"
   ],
   "id": "4ca2e876805c9f22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate # of Node Tokens",
   "id": "5767f7286d6b9887"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_name = \"Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def calculate_token_num(text):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    return len(encoded)\n",
    "\n",
    "node_token_num = []\n",
    "node_token_num_movie = []\n",
    "for i in trange(len(Index)):\n",
    "    text = Name[i]\n",
    "    node_token_num.append(calculate_token_num(text))\n",
    "    if Type[i] == 0: node_token_num_movie.append(calculate_token_num(text))\n",
    "print(len(node_token_num))"
   ],
   "id": "844f18195da5af6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "tokens = np.array(node_token_num)\n",
    "print(\"All node:\")\n",
    "print(f\"Max #node tokens: {tokens.max()}\")\n",
    "print(f\"Min #node tokens: {tokens.min()}\")\n",
    "print(f\"Avg. #node tokens: {tokens.mean()}\")\n",
    "\n",
    "tokens_movie = np.array(node_token_num_movie)\n",
    "print(\"Movie node:\")\n",
    "print(f\"Max #node tokens: {tokens_movie.max()}\")\n",
    "print(f\"Min #node tokens: {tokens_movie.min()}\")\n",
    "print(f\"Avg. #node tokens: {tokens_movie.mean()}\")\n"
   ],
   "id": "30128680e01e6d86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate # of Node Degrees",
   "id": "c3dbc8aa92fcba19"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Graph=[[] for i in range(len(Name))]\n",
    "Self_to_Self_node_count = 0\n",
    "Edge_Num = 0\n",
    "with open('./link.dat','r') as file:\n",
    "    for line in file:  \n",
    "        start,end,e_type,_=line[:-1].split('\\t')\n",
    "        Edge_Num+=1\n",
    "        if int(end) == int(start): Self_to_Self_node_count+=1\n",
    "        Graph[int(start)].append(int(end))\n",
    "    print(\"Self-Self:\", Self_to_Self_node_count)\n",
    "    print(\"# Nodes:\", len(Graph))\n",
    "    print(\"# Edges:\", Edge_Num)"
   ],
   "id": "395a69ebea7fee76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "node_degree_num = []\n",
    "node_degree_num_movie = []\n",
    "count=0\n",
    "for i in trange(len(Graph)): \n",
    "    node_degree_num.append(len(Graph[i]))\n",
    "    \n",
    "    if len(Graph[i])==0: count+=1\n",
    "    \n",
    "    if Type[i]==0: node_degree_num_movie.append(len(Graph[i]))\n",
    "    \n",
    "\n",
    "degrees_before = np.array(node_degree_num)\n",
    "degrees_before_movie = np.array(node_degree_num_movie)\n",
    "\n",
    "print(\"All node:\")\n",
    "print(f\"Max #node degrees_before: {degrees_before.max()}\")\n",
    "print(f\"Min #node degrees_before: {degrees_before.min()}\")\n",
    "print(f\"Avg. #node degrees_before: {degrees_before.mean()}\")\n",
    "\n",
    "print(f\"#node degree=0: {count}\")\n",
    "print(f\"#node degree=0/#total node: {count/len(Graph)}\")\n",
    "\n",
    "print(\"Movie node:\")\n",
    "print(f\"Max #node degrees_before: {degrees_before_movie.max()}\")\n",
    "print(f\"Min #node degrees_before: {degrees_before_movie.min()}\")\n",
    "print(f\"Avg. #node degrees_before: {degrees_before_movie.mean()}\")\n"
   ],
   "id": "c1b00a1262094706"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "degrees = np.ceil(np.log1p(degrees_before))\n",
    "\n",
    "degrees_movie = np.ceil(np.log1p(degrees_before_movie))\n",
    "\n",
    "print(\"All node:\")\n",
    "print(f\"Max #node degrees: {degrees.max()}\")\n",
    "print(f\"Min #node degrees: {degrees.min()}\")\n",
    "print(f\"Avg. #node degrees: {degrees.mean()}\")\n",
    "\n",
    "print(\"Movie node:\")\n",
    "print(f\"Max #node degrees: {degrees_movie.max()}\")\n",
    "print(f\"Min #node degrees: {degrees_movie.min()}\")\n",
    "print(f\"Avg. #node degrees: {degrees_movie.mean()}\")\n"
   ],
   "id": "f776a3f674aa9042"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Calculate Node H",
   "id": "3ec4535ce7bb49ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "H = tokens*degrees # H = T * [log(D+1)]\n",
    "H_movie = tokens_movie*degrees_movie\n",
    "\n",
    "print(\"# Nodes:\", len(H))\n",
    "print(f\"Max node H: {H.max()}\")\n",
    "print(f\"Min node H: {H.min()}\")\n",
    "print(f\"Avg. node H: {H.mean()}\")\n",
    "print(f\"Sum node H: {H.sum()}\")\n",
    "print(\"------------------------\")\n",
    "print(\"# Nodes:\", len(H_movie))\n",
    "print(f\"Max node H: {H_movie.max()}\")\n",
    "print(f\"Min node H: {H_movie.min()}\")\n",
    "print(f\"Avg. node H: {H_movie.mean()}\")\n",
    "print(f\"Sum node H: {H_movie.sum()}\")\n"
   ],
   "id": "70b501c5b6d9c343"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Node's one-hop Neighbors and Random Walks\n",
   "id": "92d9b8ec1edab543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_nodes=train_Index\n",
    "\n",
    "Neighs = {}\n",
    "for node in train_nodes: Neighs[node] = []\n",
    "Avg_degree = degrees_before_movie.mean()\n",
    "\n",
    "for i,central_node in enumerate(train_nodes):\n",
    "\n",
    "    neighs = Graph[central_node] # Neighbors\n",
    "    central_node_H = H[central_node]/2 # H*/2\n",
    "\n",
    "    neighs2H = {}\n",
    "    for j in neighs: neighs2H[j] = H[j]\n",
    "    sorted_neighs2H = dict(sorted(neighs2H.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # Obtain central node's Neighbor (#Neighbors <= Avg. #Degree)\n",
    "    Neigh_temp = []\n",
    "    for neigh, neigh_H in sorted_neighs2H.items(): \n",
    "        Neigh_temp.append(neigh)\n",
    "    if len(Neigh_temp) >= int(Avg_degree): \n",
    "        Neighs[central_node]= Neigh_temp[:int(Avg_degree)]\n",
    "        for neigh in range(int(Avg_degree),len(Neigh_temp)):\n",
    "             if sorted_neighs2H[Neigh_temp[neigh]] >= central_node_H: \n",
    "                 Neighs[central_node].append(Neigh_temp[neigh])\n",
    "    else:  Neighs[central_node] = Neigh_temp\n",
    "    \n",
    "\n",
    "count =0\n",
    "avg_count=0\n",
    "for i,central_node in enumerate(train_nodes):\n",
    "    if len(Neighs[central_node])==0: count+=1\n",
    "    avg_count+=len(Neighs[central_node])\n",
    "print(f\"#Central Node's obtained Neighbor=0: {count}\")\n",
    "print(f\"#Central Node's obtained Neighbor=0: {count/len(train_nodes)*100}%\")\n",
    "print(f\"Avg. #Central Node's obtained Neighbor: {avg_count/len(train_nodes)}\")\n",
    "print(f\"Avg. #Central Node's obtained Neighbor (#/=0): {avg_count/(len(train_nodes)-count)}\")\n"
   ],
   "id": "8207d091989b6d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdcd30d71bb1f733"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import random\n",
    "RWs = {}\n",
    "for node in train_nodes: RWs[node] = []\n",
    "start_idx = train_nodes \n",
    "\n",
    "rand=random.Random()\n",
    "max_walk_num = int(Avg_degree)+int(degrees_before_movie.min())\n",
    "alpha=0.15\n",
    "count=0\n",
    "path_length=10000\n",
    "for line in tqdm(range(len(start_idx))):\n",
    "    central_node = start_idx[line]\n",
    "    \n",
    "\n",
    "    if len(Graph[central_node])==0: \n",
    "        continue\n",
    "    \n",
    "    if len(Neighs[central_node]) < max_walk_num: \n",
    "        walk_num = max_walk_num - len(Neighs[central_node])\n",
    "    else: continue\n",
    "\n",
    "    node_paths = []\n",
    "    while len(node_paths) < walk_num:\n",
    "\n",
    "        temp_path=[]\n",
    "        start = central_node\n",
    "        temp_path.append(start)\n",
    "        \n",
    "        if len(Graph[temp_path[-1]]) == 0: \n",
    "            break \n",
    "        for i in range(path_length):\n",
    "            cur = temp_path[-1]\n",
    "            \n",
    "\n",
    "            if (len(Graph[cur])>0) and rand.random()>=alpha:\n",
    "                next_ = rand.choice(Graph[cur])\n",
    "          \n",
    "                while(rand.random()>=alpha): \n",
    "                    next_ = rand.choice(Graph[cur])\n",
    "                   \n",
    "                temp_path.append(next_)\n",
    "            else: \n",
    "                break\n",
    "        if len(temp_path)>2 and len(temp_path)<=14: \n",
    "            node_paths.append(temp_path)\n",
    "    RWs[central_node] = node_paths"
   ],
   "id": "9baf14318f6cacb9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "count =0\n",
    "avg_count=0\n",
    "for i,central_node in enumerate(train_nodes):\n",
    "    if len(RWs[central_node])==0: \n",
    "\n",
    "        count+=1\n",
    "    avg_count+=len(RWs[central_node])\n",
    "print(f\"#Central Node's obtained RWs=0: {count}\")\n",
    "print(f\"#Central Node's obtained RWs=0: {count/len(train_nodes)*100}%\")\n",
    "print(f\"Avg. #Central Node's obtained RWs: {avg_count/len(train_nodes)}\")\n",
    "print(f\"Avg. #Central Node's obtained RWs (#/=0): {avg_count/(len(train_nodes)-count)}\")"
   ],
   "id": "580aea5682a365e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b5c0ee145bf3fe8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Node Classification Instruction",
   "id": "847d5a76e82b3201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# compact graph description\n",
    "def get_subgraph(central_node, Neighs, RWs):\n",
    "    \n",
    "    Input =  \"The compact graph description of this MOVIE is listed as follows:\\n\"\n",
    "    Input += \"Title: {\" + Name[central_node] + \"}\"\n",
    "    \n",
    "    if len(Neighs)==0 and len(RWs)==0: \n",
    "        print(central_node)\n",
    "        return Input\n",
    "    else: Input += \" \"\n",
    "    \n",
    "    #---------------------------------------------------------------------\n",
    "\n",
    "    input_graph = \"Ego graph nodes: {\"\n",
    "    \n",
    "    graph_MOVIE = []\n",
    "    graph_DIRECTOR = []\n",
    "    graph_ACTOR = []\n",
    "    graph_KEYWORD = []\n",
    "    # graph_MOVIE.append(central_node)\n",
    "    \n",
    "\n",
    "    for node in Neighs: \n",
    "        if node == central_node: continue\n",
    "        \n",
    "        if Type[node] == 0: graph_MOVIE.append(node)\n",
    "        elif Type[node] == 1: graph_DIRECTOR.append(node)\n",
    "        elif Type[node] == 2: graph_ACTOR.append(node)\n",
    "        elif Type[node] == 3: graph_KEYWORD.append(node)\n",
    "    \n",
    "    # 加入RW中node信息:\n",
    "    for path in RWs:\n",
    "        for node in path:\n",
    "            if node == central_node: continue\n",
    "        \n",
    "            if Type[node] == 0: graph_MOVIE.append(node)\n",
    "            elif Type[node] == 1: graph_DIRECTOR.append(node)\n",
    "            elif Type[node] == 2: graph_ACTOR.append(node)\n",
    "            elif Type[node] == 3: graph_KEYWORD.append(node)\n",
    "    \n",
    "    tmp = []\n",
    "    tmp.append(central_node)\n",
    "    \n",
    "    graph_MOVIE = tmp + list(set(graph_MOVIE))\n",
    "    graph_DIRECTOR = list(set(graph_DIRECTOR))\n",
    "    graph_ACTOR = list(set(graph_ACTOR))\n",
    "    graph_KEYWORD = list(set(graph_KEYWORD))\n",
    "    \n",
    "\n",
    "    node2index = {}\n",
    "    count = 0\n",
    "    tmp_count = 0\n",
    "    if len(graph_MOVIE) >0:\n",
    "        count += 1\n",
    "        input_graph += \"MOVIE: [\"\n",
    "        for j,node in enumerate(graph_MOVIE):\n",
    "            if j != (len(graph_MOVIE)-1): input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\', \")\n",
    "            else: input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\']}\\n\")\n",
    "            node2index[node]=str(tmp_count+j+1)\n",
    "        tmp_count+=len(graph_MOVIE)\n",
    "    \n",
    "    if len(graph_DIRECTOR) >0:\n",
    "        count += 1\n",
    "        input_graph += \" DIRECTOR: [\"\n",
    "        for j,node in enumerate(graph_DIRECTOR):\n",
    "            if j != (len(graph_DIRECTOR)-1): input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\', \")\n",
    "            else: input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\']}\\n\")\n",
    "            node2index[node]=str(tmp_count+j+1)\n",
    "        tmp_count+=len(graph_DIRECTOR)\n",
    "    \n",
    "    if len(graph_ACTOR) >0:\n",
    "        count += 1\n",
    "        input_graph += \" ACTOR: [\"\n",
    "        for j,node in enumerate(graph_ACTOR):\n",
    "            if j != (len(graph_ACTOR)-1): input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\', \")\n",
    "            else: input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\']}\\n\")\n",
    "            node2index[node]=str(tmp_count+j+1)\n",
    "        tmp_count+=len(graph_ACTOR)\n",
    "        \n",
    "    if len(graph_KEYWORD) >0:\n",
    "        count += 1\n",
    "        input_graph += \" KEYWORD: [\"\n",
    "        for j,node in enumerate(graph_KEYWORD):\n",
    "            if j != (len(graph_KEYWORD)-1): input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\', \")\n",
    "            else: input_graph += (\"(\"+str(tmp_count+j+1)+\") \\'\" + Name[node]+\"\\']}\\n\")\n",
    "            node2index[node]=str(tmp_count+j+1)\n",
    "        tmp_count+=len(graph_KEYWORD)\n",
    "\n",
    "    Input += input_graph\n",
    "    \n",
    "    #---------------------------------------------------------------------\n",
    "    # 构造 Neighborhood information\n",
    "    input_neighborhood = \"One-hop neighbors: {\"\n",
    "    count = 0\n",
    "    for j,node in enumerate(Neighs):\n",
    "        if j!= len(Neighs)-1: input_neighborhood += (\"(\"+node2index[node] + \"), \")\n",
    "        else: input_neighborhood += (\"(\"+node2index[node] + \")}\\n\")\n",
    "    Input += input_neighborhood\n",
    "    \n",
    "    #---------------------------------------------------------------------\n",
    "    # 构造 RW\n",
    "    \n",
    "    if len(RWs)>0:\n",
    "        input_rw = \"Random walks: {\"\n",
    "        for j,path in enumerate(RWs):\n",
    "            input_rw += (chr(ord('A')+j) + \". \")\n",
    "            for k,node in enumerate(path):\n",
    "                if k != len(path)-1:\n",
    "                    temp = \"(\"+ node2index[node] + \")->\"\n",
    "                    input_rw += temp\n",
    "                else: \n",
    "                    temp = \"(\"+ node2index[node] + \")\"\n",
    "                    input_rw += temp\n",
    "            if j!= len(RWs)-1: input_rw += \"; \"\n",
    "            else: input_rw += \"}\"\n",
    "        Input += input_rw\n",
    "    return Input\n"
   ],
   "id": "80dcb661f3819729"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "type_text = \"{1. Romance 2. Thriller 3. Comedy 4. Action 5. Drama}\"\n",
    "NC_Type = [\"Romance\", \"Thriller\", \"Comedy\", \"Action\", \"Drama\"]"
   ],
   "id": "b195e218fbb12f02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "texts = []\n",
    "for i, central_node in enumerate(train_nodes):\n",
    "    text = {}\n",
    "    Instruction = \"Given the target MOVIE with the compact graph description in IMDB dataset, what the following categories does this MOVIE belong to: \" + type_text + \\\n",
    "                  \". Directly give the answer of this MOVIE‘s categories. This MOVIE may have one or more categories.\"\n",
    "    Output = \"\"\n",
    "    for l in range(len(Label[central_node])):\n",
    "        if l != len(Label[central_node]) - 1:\n",
    "            Output += str(int(Label[central_node][l]) + 1) + \". \" + NC_Type[int(Label[central_node][l])] + \", \"\n",
    "        else:\n",
    "            Output += str(int(Label[central_node][l]) + 1) + \". \" + NC_Type[int(Label[central_node][l])]\n",
    "\n",
    "    Input = get_subgraph(central_node, Neighs[central_node], RWs[central_node])\n",
    "\n",
    "    text[\"instruction\"] = Instruction\n",
    "    text[\"input\"] = Input\n",
    "    text[\"output\"] = Output\n",
    "    text[\"index\"] = int(central_node)\n",
    "\n",
    "    texts.append(text)\n"
   ],
   "id": "bd9b893e6b17d586"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import random\n",
    "# random.shuffle(texts)\n",
    "with open(\"nc_imdb.json\", \"w\") as f:\n",
    "    json.dump(texts[:], f, indent=2)\n"
   ],
   "id": "1299290261395ce8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
